{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. What is a likelihood function? Also add a formula and explain what it means. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T06:17:47.729357Z",
     "start_time": "2019-08-06T06:17:47.712005Z"
    }
   },
   "source": [
    "Likelihood function is the probability of the observed data as a function of unknown parameter.\n",
    "Mathematically, \n",
    "\\begin{equation}\n",
    "L(\\theta, \\boldsymbol{x})=\\prod_{i=1}^{n} f_{i}\\left(x_{i}\\right)=\\prod_{i=1}^{n} \\theta_{i}^{x_{i}}\\left(1-\\theta_{i}\\right)^{1-x_{i}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "where ,\n",
    "\\begin{equation}\n",
    "L(\\theta) = likelihood-function\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "x = observed-data \n",
    "\\end{equation} \n",
    "\n",
    "\\begin{equation}\n",
    "\\theta= unknown-parameter\n",
    "\\end{equation}\n",
    "\n",
    "    1.Unlike pdf, likelihoods aren’t normalized. The area under curve does not have to be 1.\n",
    "    2.We also use likelihoods to generate estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is Maximum Likelihood estimation (MLE) ? Can you give an example? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T06:27:16.321184Z",
     "start_time": "2019-08-06T06:27:16.296715Z"
    }
   },
   "source": [
    "The value of parameter (\\theta), which maximizes the likelihood function (L($\\theta$))\n",
    "is known as maximum likelihood criterion. One example of this is by finding the critical points, as maximum lies at that point, by taking derivative of likelihood function and setting it to zero.  However, if it cannot be solved analytically, we should use optimization algorithm to find the maximum likelihood.\n",
    "\n",
    "One example of maximum likelihood is flipping a coin certain(let say 100) times  and finding the maximum likelihood of having the head or tail on the next flip given the data of prior flip.\n",
    "Mathematically maximum likelihood estimate for Head is, \n",
    "\\begin{equation}\n",
    "\\hat{\\theta}_{\\mathrm{ML}}=\\frac{N_{H}}{N_{H}+N_{T}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Can gradient descent be used to find the parameters for linear regression? What about linear classification? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T07:06:01.874624Z",
     "start_time": "2019-08-06T07:06:01.839340Z"
    }
   },
   "source": [
    "Yes, gradient descent be used to find the parameters for linear regression. Since gradient descent optimization algorithm is great in finding the parameter (coefficient) if it cannot be solved using linear algebra, We can definitely use gradient descent and find the optimal parametric value of the regression line by finding the optimal value that minimizes the error function.\n",
    "It can also be used in linear classification as the cost function for linear classification is similar to linear regression and both are derived using the principle of maximum likelihood estimation.\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\theta)=\\frac{1}{m} \\sum_{i=1}^{m} \\frac{1}{2}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "we use the gradient descent algorithm and update the parameter taking the specific learning  rate and  the repititive update yields the minimal cost function and thus optimal parameter for it to occur. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. What are normal equations? Is it the same as least squares? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal equation is a technique for computing coefficients for Multivariate Linear Regression. Normal equation is used to find the best fit solution to the regression problem using least square method i.e. used to find the optimal value of unknown paramters.\n",
    "\n",
    "Mathematically,\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\mathbf{w})=\\|\\mathbf{e}\\|^{2}=\\|\\mathbf{y}-X \\mathbf{w}\\|^{2}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "w=\\left(X^{T} X\\right)^{-1} X^{T} y\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus(e). What is regularization in regression? What is the mathematical relation between regularization and normal equations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T07:57:24.780503Z",
     "start_time": "2019-08-06T07:57:24.758463Z"
    }
   },
   "source": [
    "The regression model often leads to test error when the model tries to accommodate for all kind of changes in the data including the noise. The model becomes complex having significantly high variance due to overfitting and hence impact the performance (accuracy, precision, recall, etc) of the model. Hence, we apply the regularization method (a technique to calibrate the coefficients of determination of the models in order to minimize the loss function) to reduce the variance which also ensures the model does not lead to under-fitting.\n",
    "\n",
    "Ridge regression and Lasso regression are two common Regularized linear regression methods .\n",
    "\n",
    "Mathematically, Linear regression with regularization is,\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{array}{l}{J(\\theta)=\\frac{1}{2 m}\\left[\\sum_{i=1}^{m}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right)^{2}+\\lambda \\sum_{j=1}^{n} \\theta_{j}^{2}\\right]} \\end{array}\n",
    "\\end{equation}\n",
    "\n",
    "Following is the normal equation,\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta=\\left(X^{T} X\\right)^{-1} X^{T} y\n",
    "\\end{equation}\n",
    "\n",
    "To use regularization we add a term (+ λ I(n+1*n+1)) to the equation. Then the regularized normal equation is \n",
    "\\begin{equation}\n",
    "\\theta=\\left(X^{T} X+\\lambda\\left[\\begin{array}{ccccc}{0} \\\\ {} & {1} \\\\ {} & {} & {1} \\\\ {} & {} & {} & {\\ddots} \\\\ {} & {} & {} & {} & {1}\\end{array}\\right]\\right)^{-1} X^{T} y\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
