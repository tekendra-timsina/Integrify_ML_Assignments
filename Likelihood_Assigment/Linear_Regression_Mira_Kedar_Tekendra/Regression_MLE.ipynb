{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write out MSE loss for linear regression. Could we also use this loss for classification? (qu 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE measures the average squared difference between an observation’s actual and predicted values. The output is a single number representing the cost, or score, associated with our current set of weights.\n",
    "Given our simple linear equation $y=mx+b$, we can calculate MSE as:\n",
    "$$M S E=\\frac{1}{N} \\sum_{i=1}^{n}\\left(y_{i}-\\left(m x_{i}+b\\right)\\right)^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE can certainly be calculated for forecasts or predicted values of continuous variables, but I think not for classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Maximum likelihood Estimation for linear regression. How is this related to the MSE loss for linear regression derived in the last point? (qu 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume the data is described by the linear model $\\mathbf{y} = \\mathbf{wX} + \\epsilon$, where $\\epsilon_i \\sim N(\\epsilon_i; 0,\\sigma^2_e)$. Assume $\\sigma^2_e$ is known and the datapoints are i.i.d.<b>\n",
    "The log likelihood of our model is\n",
    "\n",
    "$\\log p(\\mathbf{y}|\\mathbf{X, w}) = \\sum_{i=1}^N \\log p(y_i | \\mathbf{x_i, \\theta})$<b>\n",
    "\n",
    "But since the noise $\\epsilon$ is normally distributed, the likelihood is just<b>\n",
    "\n",
    "$\\begin{aligned} \\log p(\\mathbf{y} | \\mathbf{X}, \\mathbf{w}) &=\\sum_{i=1}^{N} \\log N\\left(y_{i} ; \\mathbf{x}_{\\mathbf{i}} \\mathbf{w}, \\sigma^{2}\\right) \\\\ &=\\sum_{i=1}^{N} \\log \\frac{1}{\\sqrt{2 \\pi \\sigma_{e}^{2}}} \\exp \\left(-\\frac{\\left(y_{i}-\\mathbf{x}_{\\mathbf{i}} \\mathbf{w}\\right)^{2}}{2 \\sigma_{c}^{2}}\\right) \\\\ &=-\\frac{N}{2} \\log 2 \\pi \\sigma_{e}^{2}-\\sum_{i=1}^{N} \\frac{\\left(y_{i}-\\mathbf{x}_{\\mathbf{i}} \\mathbf{w}\\right)^{2}}{2 \\sigma_{e}^{2}} \\end{aligned}$<b>\n",
    "\n",
    "Where N is the number of datapoints. The value of w that maximises equation is the maximum likelihood estimate w^MSE. That is,<b>\n",
    " \n",
    "$\\mathbf{w}_{M L E}=\\arg \\max _{\\mathbf{w}}-\\sum_{i=1}^{N}\\left(y_{i}-\\mathbf{x}_{\\mathbf{i}} \\mathbf{w}\\right)^{2}$<b>\n",
    "\n",
    "Recall that maximising a function is the same as minimising its negative, so we can rewrite equation as:<b>\n",
    "\n",
    "$\\mathbf{w}_{M L E}=\\arg \\min _{\\mathbf{w}} \\frac{1}{N} \\sum_{i=1}^{N}\\left(y_{i}-\\mathbf{x}_{\\mathbf{i}} \\mathbf{w}\\right)^{2}$\n",
    "$=\\arg \\min _{\\mathbf{w}} \\operatorname{MSE}$<b>\n",
    "    \n",
    "which is exactly the mean squared error (MSE) cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Write out the likelihood function for linear classification. What is the drawback of using MSE loss here? Write out the MLE approach for logistic regression. How is this related to the binary cross-entropy? (question 5+9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s consider the logistic model (a binary classifier) to describe log-odds using a linear model:\n",
    "\n",
    "$$\n",
    "\\ln \\frac{p}{1-p}=\\beta_{0}+\\beta_{1} x_{1}+\\cdots+\\beta_{m} x_{m}\n",
    "$$<b>\n",
    "\n",
    "The probability of observing outcome y=1 under this model is given by the following function (sigmoid):<b>\n",
    "$$\n",
    "p \\equiv p(y=1 | \\mathbf{B}, \\mathbf{X})=\\frac{1}{1+e^{-\\left(\\beta_{0}+\\beta_{1} x_{1}+\\cdots+\\beta_{m} x_{m}\\right)}}\n",
    "$$<b>\n",
    "\n",
    "With 0 and 1 being the only possible outcomes, the probability of observing outcome y=0 is simply (1-p):\n",
    "\n",
    "$$\n",
    "p(y=o | \\mathbf{B}, \\mathbf{X})=p^{o}(1-p)^{1-o}\n",
    "$$\n",
    "\n",
    "The likelihood function is given by the product of all individual probabilities:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}=\\prod_{i=1}^{n} p\\left(y=y_{i} | \\mathbf{B}_{i}, \\mathbf{X}_{i}\\right)=\\prod_{i=1}^{n} p_{i}^{y_{i}}\\left(1-p_{i}\\right)^{1-y_{i}}\n",
    "$$\n",
    "\n",
    "It’s easier to maximize the log-likelihood:\n",
    "\n",
    "$$\n",
    "\\ln \\mathcal{L}=\\sum_{i=1}^{n}\\left(y_{i} \\ln p_{i}+\\left(1-y_{i}\\right) \\ln \\left(1-p_{i}\\right)\\right)\n",
    "$$\n",
    "Thus  maximum liklihood estimation yields a familiar loss function (cross-entropy in this case).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the basics of statistical machine learning, the loss function is the negative-log-likelihood (NLL) of the model. To find the best parameters, we need to minimize this NLL loss. The MSE is the NLL for the linear regression model (with Gaussian likelihood). But for the logistic regression model, the NLL is the cross-entropy (with Bernoulli likelihood). So minimizing the MSE loss for a logistic regression model is minimizing something else other than the NLL of that model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Is feature scaling needed for linear regression when using gradient descent? (qu 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, feature scaling is needed for linear regression beecause it speeds up gradient descent by scaling. This is because θ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven. It speeds up gradient descent by making it require fewer iterations to get to a good solution. It is necessary to prevent gradient descent from getting stuck in local optima.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Prove that the sigmoid function is symmetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The domain of the sigmoid function is the set of all real numbers, R , and it's defined as:\n",
    "\n",
    "$$\n",
    "\\sigma(x)=\\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "\n",
    "From equation it is clear that \n",
    "x\n",
    " gets larger the value of \n",
    "σ\n",
    "(\n",
    "x\n",
    ")\n",
    " tends towards \n",
    "1\n",
    "*. Similarly it should be evident that the limit of \n",
    "σ\n",
    "(\n",
    "x\n",
    ")\n",
    ", as \n",
    "x\n",
    " approaches negative infinity, is \n",
    "0\n",
    ". That is:\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}{\\lim _{x \\rightarrow \\infty} \\sigma(x)=1} \\\\ {\\lim _{x \\rightarrow-\\infty} \\sigma(x)=0}\\end{array}\n",
    "$$\n",
    "\n",
    "I.e. the range of \n",
    "σ\n",
    "(\n",
    "x\n",
    ")\n",
    " are real numbers in the interval \n",
    "[\n",
    "0\n",
    ",\n",
    "1\n",
    "]\n",
    ", and there's a \"soft step\" between the off and on values represented by the extremes of its range. As mentioned above the sigmoid function is a function with domain over all \n",
    "R\n",
    ", so we can sum this up as:\n",
    "\n",
    "σ\n",
    ":\n",
    "R\n",
    "→\n",
    "[\n",
    "0\n",
    ",\n",
    "1\n",
    "]\n",
    "Also note that \n",
    "σ\n",
    "(\n",
    "x\n",
    ")\n",
    " has rotational symmetry with respect to the point \n",
    "(\n",
    "0\n",
    ",\n",
    "1\n",
    "2\n",
    ")\n",
    ", and the sum of the sigmoid function and its reflection about the vertical axis, \n",
    "σ\n",
    "(\n",
    "−\n",
    "x\n",
    ")\n",
    " is\n",
    " \n",
    "$$\n",
    "\\sigma(x)+\\sigma(-x)=1\n",
    "$$\n",
    "Now, prove it detail:\n",
    "\n",
    "$\\begin{aligned} \\sigma(x)+\\sigma(-x) &=\\frac{1}{1+e^{-x}}+\\frac{1}{1+e^{-(-x)}} \\\\ &=\\left(\\frac{1+e^{x}}{1+e^{x}}\\right) \\cdot \\frac{1}{1+e^{-x}}+\\left(\\frac{1+e^{-x}}{1+e^{-x}}\\right) \\cdot \\frac{1}{1+e^{x}} \\\\ &=\\frac{\\left(1+e^{-x}\\right)+\\left(1+e^{-x}\\right)}{\\left(1+e^{-x}\\right) \\cdot\\left(1+e^{x}\\right)} \\\\ &=\\frac{2+e^{x}+e^{-x}}{1+e^{x}+e^{-x}+e^{-x+x}}=\\frac{2+e^{x}+e^{-x}}{2+e^{x}+e^{-x}} \\\\ &=1 \\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) when to use normal equation\n",
    "\n",
    "If n is relatively small ( on the order of a hundred ~ ten thousand ), then the normal equation is more efficent because in gradient descent we need to choose the learning rate, so it needs to run the algorithm at least a few times to figure that out. It needs many more iterations, so, that could make it slower. On the other hand, Normal Equation is computationally expensive when we have a very large number of features ( n features ), because we will ultimately need to take the inverse of a n x n matrix in order to solve for the parameters data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Is feature scaling  needed for normal equation?\n",
    "In theory, when using normal equations, we don’t use numerical methods, like gradient descent. Therefore, feature scaling is not necessary. However, some of the libraries may use to compute inverses in the normal equation might themselves be using some numerical methods to compute the inverse, and so, feature scaling might still be helpful in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
